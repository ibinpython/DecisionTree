**决策树算法** 
		决策树在构造过程中不需要任何领域知识或参数设置，因此在实际应用中，对于探测式的知识发现，决策树更加适用。 

**1、信息增益、信息熵** 
ID3决策树学习算法就是以“信息增益”(information gain)为准则来选择划分属性。 
信息增益准则对可取值数目较多的属性有所偏好（因为相对来说，每个分支结点下样本越少，纯度越高）。 
**2、增益率** 
C4.5决策树算法使用“增益率”(gain ratio)来选择最优划分属性。 
增益率准则对可取值数目较少的属性有所偏好，因此，C4.5算法不直接选择增益率最大的候选属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。 
**3、基尼指数** 
CART决策树使用“基尼指数”(Gini index)来选择划分属性。 
Gini(D)反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。因此Gini(D)越小，则数据集的纯度越高。

**常见决策树分类算法：**

(1)、CLS算法：是最原始的决策树分类算法，基本流程是，从一棵空数出发，不断的从决策表选取属性加入数的生长过程中，直到决策树可以满足分类要求为止。CLS算法存在的主要问题是在新增属性选取时有很大的随机性。

(2)、ID3算法：对CLS算法的最大改进是摒弃了属性选择的随机性，利用信息熵的下降速度作为属性选择的度量。ID3是一种基于信息熵的决策树分类学习算法，以信息增益和信息熵，作为对象分类的衡量标准。ID3算法结构简单、学习能力强、分类速度快适合大规模数据分类。但同时由于信息增益的不稳定性，容易倾向于众数属性导致过度拟合，算法抗干扰能力差。

ID3算法的核心思想：根据样本子集属性取值的信息增益值的大小来选择决策属性(即决策树的非叶子结点)，并根据该属性的不同取值生成决策树的分支，再对子集进行递归调用该方法，当所有子集的数据都只包含于同一个类别时结束。最后，根据生成的决策树模型，对新的、未知类别的数据对象进行分类。

ID3算法优点：方法简单、计算量小、理论清晰、学习能力较强、比较适用于处理规模较大的学习问题。

ID3算法缺点：倾向于选择那些属性取值比较多的属性，在实际的应用中往往取值比较多的属性对分类没有太大价值、不能对连续属性进行处理、对噪声数据比较敏感、需计算每一个属性的信息增益值、计算代价较高。

(3)、C4.5算法：基于ID3算法的改进，主要包括：使用信息增益率替换了信息增益下降度作为属性选择的标准；在决策树构造的同时进行剪枝操作；避免了树的过度拟合情况；可以对不完整属性和连续型数据进行处理；使用k交叉验证降低了计算复杂度；针对数据构成形式，提升了算法的普适性。

(4)、SLIQ算法：该算法具有高可扩展性和高可伸缩性特质，适合对大型数据集进行处理。

(5)、CART(Classification and RegressionTrees, CART)算法：是一种二分递归分割技术，把当前样本划分为两个子样本，使得生成的每个非叶子节点都有两个分支，因此，CART算法生成的决策树是结构简洁的二叉树。

分类回归树算法(Classification and Regression Trees,简称CART算法)是一种基于二分递归分割技术的算法。该算法是将当前的样本集，分为两个样本子集，这样做就使得每一个非叶子节点最多只有两个分支。因此，使用CART算法所建立的决策树是一棵二叉树，树的结构简单，与其它决策树算法相比，由该算法生成的决策树模型分类规则较少。

CART分类算法的基本思想是：对训练样本集进行递归划分自变量空间，并依次建立决策树模型，然后采用验证数据的方法进行树枝修剪，从而得到一颗符合要求的决策树分类模型。

**决策树的解释** 
1、决策树是基于树结构进行决策的 
2、一般地，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点。叶结点对应于决策结果，其他每个结点则对应于一个属性测试。 
3、每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。 
4、从根结点到每个叶结点的路径对应了一个判定测试序列。

**决策树的构建** 
1、决策树学习的目的是为了产生一颗泛化能力强，即处理未见示例能力强的决策树。 
2、决策树的生成是一个递归的过程。 
在决策树基本算法中，有3种情形会导致递归返回： 
（1）当前结点包含的样本全属于同一类别，无需划分； 
（2）当前结点包含的样本集合为空，不能划分； 
（3）当前属性集为空，或是所有样本在所有属性上取值相同，无法划分。 
但是，这样往往会使得树的节点过多，导致过拟合问题。 
可行的方法是增加停止条件：1）当前结点中的记录数低于一个最小的阈值，那么久停止分割。2）设置树的最大深度。

**划分属性** 
		贪心算法，使决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”(purity)越来越高。 
划分属性分为三种不同的情况： 
1）属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。 
2）属性是离散值且要求生成二叉决策树。此时用属性划分的一个子集进行测试，按照“属于此子集“和”不属于此子集“分成两个分支。 
3）属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。 

**决策树防止过拟合手段**：剪枝处理 
		在决策树学习中，为了尽可能正确分类训练样本，节点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得“太好了”，以至于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。 
决策树剪枝的基本策略：“预剪枝”(prepruning)和“后剪枝”(postpruning)

**连续值处理** 
1、C4.5决策树算法中采用二分法离散化连续属性。 
需要注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性。

缺失值处理 
C4.5算法有该处理。 
基本思想是考虑非缺失样例的信息增益，然后乘上非缺失值的比例。但是这个比例的定义为非缺失值的权重之和除以全部数据的权重之和。

单变量决策树，在每个非叶结点仅考虑一个划分属性，产生“轴平行”分类面。即分类边界的每一段都是与坐标轴平行的。 

多变量决策树(multivariate decision tree)能够实现“斜划分”甚至更复杂的决策树。

sklearn参数详解，Python绘制决策树

class sklearn.tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)
criterion：划分衡量指标

splitter：搜索划分方式（最优/随机）

max_depth：决策树最大深度

min_samples_split：决策树叶结点继续分裂最小样本数量

min_samples_leaf：决策树叶结点最小样本数量

min_weight_fraction_leaf：决策树叶结点最小加权样本数量

max_features：搜索划分时考虑的特征数量

random_state：随机种子

max_leaf_nodes：决策树最大叶结点数量

min_impurity_decrease：决策树叶结点最小衡量指标提升

class_weight：类别权重

presort：是否尝试预先排序数据

DecisionTreeRegressor类似

决策树绘制：sklearn.tree.plot_tree

参考：https://blog.csdn.net/MarinkaWang/article/details/51210470